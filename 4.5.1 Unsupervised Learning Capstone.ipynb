{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4.5.1 Unsupervised Learning Capstone\n",
    "For this project you'll dig into a large amount of text and apply most of what you've covered in this unit and in the course so far.\n",
    "\n",
    "First, pick a set of texts. This can be either a series of novels, chapters, or articles. Anything you'd like. It just has to have multiple entries of varying characteristics. At least 100 should be good. There should also be at least 10 different authors, but try to keep the texts related (either all on the same topic of from the same branch of literature - something to make classification a bit more difficult than obviously different subjects).\n",
    "\n",
    "This capstone can be an extension of your NLP challenge if you wish to use the same corpus. If you found problems with that data set that limited your analysis, however, it may be worth using what you learned to choose a new corpus. Reserve 25% of your corpus as a test set.\n",
    "\n",
    "The first technique is to create a series of clusters. Try several techniques and pick the one you think best represents your data. Make sure there is a narrative and reasoning around why you have chosen the given clusters. Are authors consistently grouped into the same cluster?\n",
    "\n",
    "Next, perform some unsupervised feature generation and selection using the techniques covered in this unit and elsewhere in the course. Using those features then build models to attempt to classify your texts by author. Try different permutations of unsupervised and supervised techniques to see which combinations have the best performance.\n",
    "\n",
    "Lastly return to your holdout group. Does your clustering on those members perform as you'd expect? Have your clusters remained stable or changed dramatically? What about your model? Is it's performance consistent?\n",
    "\n",
    "If there is a divergence in the relative stability of your model and your clusters, delve into why.\n",
    "\n",
    "Your end result should be a write up of how clustering and modeling compare for classifying your texts. What are the advantages of each? Why would you want to use one over the other? Approximately 3-5 pages is a good length for your write up, and remember to include visuals to help tell your story!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Necessary imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import requests\n",
    "import re\n",
    "import spacy\n",
    "import sklearn\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Text Selection and Cleaning\n",
    "For my selected texts, I decided to use the New York Times article API to get a selection of texts, all from December, every 10 years from 1960 to 2010. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nyt_api = '5cb4f9a5273b4fbf97ef0d7d01eb6273'\n",
    "# Get Requests to pull JSON data\n",
    "request_2010 = requests.get('http://api.nytimes.com/svc/archive/v1/2010/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_2000 = requests.get('http://api.nytimes.com/svc/archive/v1/2000/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1990 = requests.get('http://api.nytimes.com/svc/archive/v1/1990/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1980 = requests.get('http://api.nytimes.com/svc/archive/v1/1980/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1970 = requests.get('http://api.nytimes.com/svc/archive/v1/1970/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "request_1960 = requests.get('http://api.nytimes.com/svc/archive/v1/1960/12.json?api-key=5cb4f9a5273b4fbf97ef0d7d01eb6273')\n",
    "# Gathering responses from JSON data\n",
    "response_2010 = request_2010.json()\n",
    "response_2000 = request_2000.json()\n",
    "response_1990 = request_1990.json()\n",
    "response_1980 = request_1980.json()\n",
    "response_1970 = request_1970.json()\n",
    "response_1960 = request_1960.json()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Selecting document information from JSON\n",
    "docs_2010 = response_2010['response']['docs']\n",
    "docs_2000 = response_2000['response']['docs']\n",
    "docs_1990 = response_1990['response']['docs']\n",
    "docs_1980 = response_1980['response']['docs']\n",
    "docs_1970 = response_1970['response']['docs']\n",
    "docs_1960 = response_1960['response']['docs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great, now that I've gathered the texts, let's see a sampling of a lead paragraph to see what we're getting into."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The best-selling novelist Brad Meltzer leads a team of investigators in exploring mysteries of American history.']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[docs_2010[2]['lead_paragraph']]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let's extract the lead paragraph from each of the first 100 articles from each year. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "nyt_2010 = []\n",
    "for article in docs_2010[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_2010.append([art[i], '2010'])\n",
    "\n",
    "nyt_2000 = []\n",
    "for article in docs_2000[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_2000.append([art[i], '2000'])\n",
    "\n",
    "nyt_1990 = []\n",
    "for article in docs_1990[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1990.append([art[i], '1990'])\n",
    "\n",
    "nyt_1980 = []\n",
    "for article in docs_1980[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1980.append([art[i], '1980'])\n",
    "\n",
    "nyt_1970 = []\n",
    "for article in docs_1970[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1970.append([art[i], '1970'])\n",
    "        \n",
    "nyt_1960 = []\n",
    "for article in docs_1960[0:100]:\n",
    "    art = [article['lead_paragraph']]\n",
    "    for i in range(len(art)):\n",
    "        nyt_1960.append([art[i], '1960'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have gathered the information and labeled each with its respective year published, let's combine all of these years into one data frame to then be able to manipulate and use for analysis and modeling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Boulder’s Uptown, with its new shops and resta...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>With 10 nods for his comeback album, \"Recovery...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>The best-selling novelist Brad Meltzer leads a...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Nicholas D. Kristof visits a Haitian cholera t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nicholas D. Kristof reports from Haiti about t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      lead_paragraph  year\n",
       "0  Boulder’s Uptown, with its new shops and resta...  2010\n",
       "1  With 10 nods for his comeback album, \"Recovery...  2010\n",
       "2  The best-selling novelist Brad Meltzer leads a...  2010\n",
       "3  Nicholas D. Kristof visits a Haitian cholera t...  2010\n",
       "4  Nicholas D. Kristof reports from Haiti about t...  2010"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "years = [nyt_2010, nyt_2000, nyt_1990, nyt_1980, nyt_1970, nyt_1960]\n",
    "nyt_all = pd.DataFrame(columns=['lead_paragraph', 'year'])\n",
    "for year in years:\n",
    "    for i in range(len(nyt_2010)):\n",
    "        nyt_all = nyt_all.append({'lead_paragraph':year[i][0], 'year':year[i][1]}, ignore_index=True)\n",
    "nyt_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And in order to best analyze these lead paragaphs, let's define a function to clean out the double dashes, which will not be able to be processed by natural language processing, remove all numbers, which will not provide any decade relevant information, take the lowercase of all words for consistency, and then remove all extra white space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text):\n",
    "    text = re.sub(r'--', ' ', text)\n",
    "    text = re.sub(r'\\d', '', text)\n",
    "    #text = re.sub(r'\\.', '. ', text)\n",
    "    text = text.lower()\n",
    "    text = ' '.join(text.split())\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lead_paragraph</th>\n",
       "      <th>year</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>boulder’s uptown, with its new shops and resta...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>with nods for his comeback album, \"recovery,\" ...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>the best-selling novelist brad meltzer leads a...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>nicholas d. kristof visits a haitian cholera t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nicholas d. kristof reports from haiti about t...</td>\n",
       "      <td>2010</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                      lead_paragraph  year\n",
       "0  boulder’s uptown, with its new shops and resta...  2010\n",
       "1  with nods for his comeback album, \"recovery,\" ...  2010\n",
       "2  the best-selling novelist brad meltzer leads a...  2010\n",
       "3  nicholas d. kristof visits a haitian cholera t...  2010\n",
       "4  nicholas d. kristof reports from haiti about t...  2010"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Clean all lead paragraphs\n",
    "nyt_all['lead_paragraph'] = nyt_all.lead_paragraph.map(lambda x: text_cleaner(str(x)))\n",
    "nyt_all.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, we will set aside 25% of the corpus as a test set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Identifying variables\n",
    "X = nyt_all['lead_paragraph']\n",
    "y = nyt_all['year']\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our lead paragraphs labeled and cleaned, we are ready for further analysis of these texts.\n",
    "\n",
    "## Turning Words into Values\n",
    "### Tf-idf Vectorization\n",
    "To best cluster our data, we will first turn our paragraphs into vectors.  Term Frequency Inverse Document Frequency (Tf-idf) vectorization takes into account how many times a particular word appears in a document and then takes into account the infrequent words to create a vector for each individual word. \n",
    "\n",
    "For this vectorizer, we will use the following parameters:\n",
    "- Drop the words that occur in more than half the paragraphs\n",
    "- Only use words that appear at least 4 times\n",
    "- Drop English stop words\n",
    "- Leave the text in lowercase, which was already cleaned out\n",
    "- Apply a correction factor so that longer paragraphs and shorter paragraphs get treated equally\n",
    "- Add 1 to all document frequencies to prevent divide-by-zero errors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of features: 216\n",
      "Original sentence: buffalo, nov. (upi) a blizzard-led storm system crippled transportation in upstate cities today.\n",
      "Tf_idf vector: {'nov': 0.7002728551885559, 'today': 0.7138752890288806}\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_df=0.8,\n",
    "                            min_df=10,\n",
    "                            stop_words='english',\n",
    "                            lowercase=False,\n",
    "                            use_idf=True,\n",
    "                            norm=u'l2',\n",
    "                            smooth_idf=True)\n",
    "# Applying the vectorizer\n",
    "X_tfidf = vectorizer.fit_transform(X)\n",
    "print('Number of features: {}'.format(X_tfidf.get_shape()[1]))\n",
    "\n",
    "# Splitting into train and test sets\n",
    "X_train_tfidf, X_test_tfidf, y_train, y_test = train_test_split(X_tfidf, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Reshape vectorizer to readable content\n",
    "X_train_tfidf_csr = X_train_tfidf.tocsr()\n",
    "\n",
    "# Number of paragraphs\n",
    "n = X_train_tfidf_csr.shape[0]\n",
    "\n",
    "# A list of dictionaries, one per paragraph\n",
    "tfidf_bypara = [{} for _ in range(0,n)]\n",
    "\n",
    "# List of features\n",
    "terms = vectorizer.get_feature_names()\n",
    "\n",
    "# For each paragraph, lists the feature words and their tf-idf scores\n",
    "for i, j in zip(*X_train_tfidf_csr.nonzero()):\n",
    "    tfidf_bypara[i][terms[j]] = X_train_tfidf_csr[i, j]\n",
    "\n",
    "# Keep in mind that the log base 2 of 1 is 0, so a tf-idf score of 0 indicates that the word was present once in that sentence.\n",
    "print('Original sentence:', X_train[593])\n",
    "print('Tf_idf vector:', tfidf_bypara[0])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dimension Reduction\n",
    "Now, in order to best cluster with k-means, we will use dimensionality reduction in the form of singlar value decomposition to reduce the feature space. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percent variance captured by all components: 81.14186195750321\n"
     ]
    }
   ],
   "source": [
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "# SVD data reducer.  We are going to reduce the feature space from 1599 to 150.\n",
    "svd = TruncatedSVD(100)\n",
    "lsa = make_pipeline(svd, Normalizer(copy=False))\n",
    "# Run SVD on the training data, then project the training data.\n",
    "X_train_lsa = lsa.fit_transform(X_train_tfidf)\n",
    "\n",
    "total_variance = svd.explained_variance_ratio_.sum()\n",
    "print(\"Percent variance captured by all components:\",total_variance*100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This looks like the model is doing a good job of capturing the variance in the components. Now we're ready for clustering."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Clustering\n",
    "We will first try to create a series of clusters to group the paragraphs to see if the clusters group according to decades or other themes.  We will explore a couple of different clustering methods to see which best models the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### K-Means Clustering\n",
    "We will start with K-Means clustering, where each point will be clustered based on minimizing the inertia, or sum of squared differences between the mean of the cluster and the data points of the cluster. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>9</td>\n",
       "      <td>46</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>6</td>\n",
       "      <td>38</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>24</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>9</td>\n",
       "      <td>30</td>\n",
       "      <td>6</td>\n",
       "      <td>7</td>\n",
       "      <td>17</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>11</td>\n",
       "      <td>18</td>\n",
       "      <td>5</td>\n",
       "      <td>16</td>\n",
       "      <td>27</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>8</td>\n",
       "      <td>21</td>\n",
       "      <td>3</td>\n",
       "      <td>14</td>\n",
       "      <td>18</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>3</td>\n",
       "      <td>46</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>7</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1  2   3   4   5\n",
       "year                        \n",
       "1960    9  46  8   0   0   9\n",
       "1970    6  38  9   0   2  24\n",
       "1980    9  30  6   7  17  12\n",
       "1990   11  18  5  16  27   1\n",
       "2000    8  21  3  14  18   8\n",
       "2010    3  46  0  10   7   2"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Calulate predicted values\n",
    "kmeans = KMeans(n_clusters=6, init='k-means++', random_state=42, n_init=10)\n",
    "y_pred = kmeans.fit_predict(X_train_tfidf)\n",
    "\n",
    "pd.crosstab(y_train, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.04835364\n",
      "Silhouette Score: 0.06748597\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import adjusted_rand_score\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_train_lsa, y_pred, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the adjusted rand score and silhouette score, this model is esentially randomly assigning paragraphs into clusters, which isn't good.  Let's try another model.\n",
    "\n",
    "### Mini Batch K Means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>63</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>67</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>60</td>\n",
       "      <td>7</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>56</td>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>57</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>63</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1  2  3  4  5  6\n",
       "year                        \n",
       "1960   63   0  0  0  0  9  0\n",
       "1970   67   5  0  0  2  5  0\n",
       "1980   60   7  6  0  3  5  0\n",
       "1990   56  10  2  2  3  4  1\n",
       "2000   57   1  1  3  3  6  1\n",
       "2010   63   1  1  0  1  2  0"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import MiniBatchKMeans\n",
    "minikmeans = MiniBatchKMeans(n_clusters=7, init='k-means++', random_state=42, init_size=1000, batch_size=1000)\n",
    "y_pred2 = minikmeans.fit_predict(X_train_lsa)\n",
    "\n",
    "pd.crosstab(y_train, y_pred2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.0004316068\n",
      "Silhouette Score: 0.0599363\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred2)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_train_lsa, y_pred2, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Again, this model is essentially assigning at random.  Let's try again.\n",
    "\n",
    "### Spectral Clustering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 6\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>21</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>8</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>22</td>\n",
       "      <td>13</td>\n",
       "      <td>1</td>\n",
       "      <td>16</td>\n",
       "      <td>5</td>\n",
       "      <td>22</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>47</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>10</td>\n",
       "      <td>4</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>49</td>\n",
       "      <td>15</td>\n",
       "      <td>8</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>45</td>\n",
       "      <td>3</td>\n",
       "      <td>9</td>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>56</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0   0   1  2   3  4   5\n",
       "year                       \n",
       "1960   21   4  1   6  8  32\n",
       "1970   22  13  1  16  5  22\n",
       "1980   47   9  3  10  4   8\n",
       "1990   49  15  8   1  4   1\n",
       "2000   45   3  9   6  6   3\n",
       "2010   56   1  4   0  2   5"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import SpectralClustering\n",
    "\n",
    "n_clusters= 6\n",
    "sc = SpectralClustering(n_clusters=n_clusters)\n",
    "y_pred3 = sc.fit_predict(X_train_lsa)\n",
    "\n",
    "pd.crosstab(y_train, y_pred3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.05072691\n",
      "Silhouette Score: -0.04634463\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred3)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_train_lsa, y_pred3, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This has an ever-so-slightly higher adjusted rand score, but this is still no different than random assignments.  Let's try a final model.\n",
    "\n",
    "### Affinity Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of estimated clusters: 22\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>col_0</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>12</th>\n",
       "      <th>13</th>\n",
       "      <th>14</th>\n",
       "      <th>15</th>\n",
       "      <th>16</th>\n",
       "      <th>17</th>\n",
       "      <th>18</th>\n",
       "      <th>19</th>\n",
       "      <th>20</th>\n",
       "      <th>21</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>year</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1960</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>57</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1970</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>49</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1980</th>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1990</th>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>62</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2000</th>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>52</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2010</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>55</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>6 rows × 22 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "col_0  0   1   2   3   4   5   6   7   8   9  ...  12  13  14  15  16  17  18  \\\n",
       "year                                          ...                               \n",
       "1960    0   0   1   0   1   0   0  57   0   0 ...   1   1   0   3   0   0   0   \n",
       "1970    0   0   2   1   2   3   2  49   0   2 ...   3   0   0   2   2   1   0   \n",
       "1980    2   0   0   0   2   1   0  52   1   4 ...   0   0   4   1   1   2   2   \n",
       "1990    3   2   0   0   0   1   0  62   0   4 ...   1   1   0   1   0   2   0   \n",
       "2000    2   2   0   6   0   0   0  52   1   1 ...   0   1   0   1   0   1   1   \n",
       "2010    1   0   0   0   0   0   0  55   2   0 ...   0   3   1   0   2   0   1   \n",
       "\n",
       "col_0  19  20  21  \n",
       "year               \n",
       "1960    0   0   5  \n",
       "1970    1   1   3  \n",
       "1980    1   2   3  \n",
       "1990    1   0   0  \n",
       "2000    1   1   2  \n",
       "2010    0   0   1  \n",
       "\n",
       "[6 rows x 22 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.cluster import AffinityPropagation\n",
    "\n",
    "af = AffinityPropagation(damping=0.6, max_iter=550, copy=False)\n",
    "y_pred4 = af.fit_predict(X_train_lsa)\n",
    "\n",
    "cluster_centers_indices = af.cluster_centers_indices_\n",
    "n_clusters = len(cluster_centers_indices)\n",
    "print('Number of estimated clusters: {}'.format(n_clusters))\n",
    "\n",
    "pd.crosstab(y_train, y_pred4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Adjusted Rand Score: 0.000568541\n",
      "Silhouette Score: 0.07617274\n"
     ]
    }
   ],
   "source": [
    "print('Adjusted Rand Score: {:0.7}'.format(adjusted_rand_score(y_train, y_pred4)))\n",
    "print('Silhouette Score: {:0.7}'.format(silhouette_score(X_train_lsa, y_pred4, metric='euclidean')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "None of these are better than randomly choosing clusters for each paragraph, even with trying to optimize parameters. This might be because the same types of words are used in each decade, and the specific events weren't covered enough in the data set to be able to distinguish the difference."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Generation and Selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['best', 'sell', 'novelist', 'brad', 'meltzer', 'lead', 'team', 'investigator', 'explore', 'mystery', 'american', 'history']\n"
     ]
    }
   ],
   "source": [
    "nlp = spacy.load('en')\n",
    "\n",
    "years = [nyt_2010, nyt_2000, nyt_1990, nyt_1980, nyt_1970, nyt_1960]\n",
    "sentences = []\n",
    "for year in years:\n",
    "    for i in range(len(year)):\n",
    "        nyt_clean = text_cleaner(str(year[i][0]))\n",
    "        nyt_nlp_sent = nlp(nyt_clean)\n",
    "        for sentence in nyt_nlp_sent.sents:\n",
    "            sentence = [\n",
    "                token.lemma_.lower()\n",
    "                for token in sentence\n",
    "                if not token.is_stop\n",
    "                and not token.is_punct\n",
    "            ]\n",
    "            sentences.append(sentence)\n",
    "\n",
    "print(sentences[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done!\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "from gensim.models import word2vec\n",
    "\n",
    "model = word2vec.Word2Vec(\n",
    "    sentences,\n",
    "    workers=2,     # Number of threads to run in parallel (if your computer does parallel processing).\n",
    "    min_count=10,  # Minimum word count threshold.\n",
    "    window=3,      # Number of words around target word to consider.\n",
    "    sg=0,          # Use CBOW because our corpus is small.\n",
    "    sample=1e-3 ,  # Penalize frequent words.\n",
    "    size=300,      # Word vector length.\n",
    "    hs=1           # Use hierarchical softmax.\n",
    ")\n",
    "\n",
    "print('done!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['new', 'visit', 'lead', 'award', 'announce', 'wednesday', 'night', 'sell', 'team', 'american', 'history', 'treatment', 'center', 'report', 'problem', 'provide', 'help', 'tuesday', \"'s\", 'meeting', 'yankee', 'set', 'close', 'call', 'suggest', 'contract', 'health', 'organization', 'year', 'little', 'exchange', 'service', 'week', 'be', 'work', 'time', 'day', 'world', 'field', 'live', 'kid', 'speak', 'design', 'name', 'sunday', 'season', 'university', 'want', '$', 'million', 'art', 'money', 'plan', 'court', 'decision', 'sale', 'billion', 'high', 'price', 'pay', 'company', 'like', 'cut', 'rate', 'deal', 'continue', 'point', 'issue', 'long', 'right', 'require', 'rule', 'house', 'republican', 'national', 'committee', 'place', 'leave', 'month', 'investigation', 'water', 'democrat', 'bank', 'share', 'base', 'large', 'united', 'state', 'make', 'stock', 'see', 'york', 'open', 'big', 'san', 'area', 'west', 'south', 'seek', 'party', 'need', 'fund', 'network', 'hope', 'president', 'east', 'conference', 'begin', 'include', 'football', 'find', 'list', 'research', 'get', 'low', 'page', 'radio', 'television', 'news', 'man', 'review', 'article', 'good', 'today', 'bring', 'ago', 'game', 'not', 'morning', 'nov', 'f.', 'investment', 'chairman', 'group', 'send', 'spend', 'school', 'queen', 'public', 'city', 'bill', 'turn', 'movie', 'try', 'federal', 'loan', 'crisis', 'market', 'order', 'department', 'clear', 'senator', 'john', 'effort', 'artist', 'use', 'performance', 'series', 'follow', 'november', 'government', 'street', 'industry', 'p.m.', 'senate', 'way', 'control', 'vote', 'leader', 'possible', 'member', 'a.', 'write', 'theater', 'production', 'short', 'play', 'foreign', 'great', 'nation', 'say', 'stand', 'film', 'director', 'nuclear', 'administration', '-year', 'arm', 'mean', 'parent', 'fall', 'family', 'lose', 'survive', 'present', 'dec', 'bear', 'oil', 'hour', 'near', 'b.', 'ask', 'talk', 'star', 'friday', 'young', 'come', 'look', 'different', 'know', 'st', 'jame', 'think', 'executive', 'second', 'hold', 'release', 'show', 'local', 'benefit', 'food', 'product', 'start', 'paul', 'end', 'line', 'coach', 'record', 'social', 'aid', 'people', 'david', 'return', 'story', 'war', 'nearly', 'free', 'head', 'future', 'level', 'offer', 'division', 'acquire', 'recent', 'hard', 'receive', 'room', 'white', 'magazine', 'manhattan', 'japan', 'number', 'elect', 'association', 'loss', 'election', 'take', 'add', 'title', 'december', 'cost', 'expect', 'thursday', 'bush', 'air', 'have', 'press', 'major', 'life', 'el', 'power', 'miami', 'beach', 'saturday', 'security', 'mayor', 'brooklyn', 'police', 'housing', 'case', 'old', 'boston', 'chief', 'give', 'mr', 'decide', 'drug', 'test', 'job', 'political', 'date', 'private', 'reach', 'board', 'th', 'florida', 'session', 'early', 'late', 'role', 'washington', 'win', 'yesterday', 'average', 'child', 'october', 'quarter', 'editor', 'action', 'die', 'law', 'daughter', 'seven', 'funeral', 'far', 'william', 'military', 'international', 'approve', 'minister', 'victory', 'face', 'go', 'tonight', 'single', 'economy', 'percent', 'increase', 'business', 'half', 'weapon', 'battle', 'mark', 'society', 'move', 'tomorrow', 'manager', 'home', 'agree', 'remain', 'union', 'rise', 'term', 'general', 'inc', 'program', 'kill', 'run', 'charge', 'question', 'congress', 'tell', 'richard', 'judge', 'minute', 'wife', 'love', 'hospital', 'medical', 'woman', 'feel', 'fight', 'robert', 'county', 'meet', 'north', 'j.', 'official', 'change', 'index', 'corporation', 'pass', 'interest', 'league', 'park', 'policy', 'result', 'country', 'central', 'jewish', 'gulf', 'plant', 'discuss', 'schwartz', 'player', 'mrs', 'office', 'council', 'german', 'worker', 'force', 'agency', 'son', 'l.i', 'community', 'iraq', 'kuwait', 'saddam', 'hussein', 'vietnam', 'berlin', 'cent', 'd', 'secretary', 'ap', 'soviet', 'score', 'resolution', 'embassy', 'goal', 'tie', 'q.', 'touchdown', 'yard', '-yard', 'reagan'])"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# List of words in model.\n",
    "vocab = model.wv.vocab.keys()\n",
    "vocab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
